{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_allclose \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M =1\n",
    "K=8\n",
    "N=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the matrices\n",
    "A = np.random.rand(M, K).astype(np.float32)  \n",
    "B = np.random.rand(N, K).astype(np.float32)   #  transposed \n",
    "C = np.random.rand(M, N).astype(np.float32)  # 10x8 matrix\n",
    "\n",
    "# Parameters for GEMM\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "\n",
    "# Define the split point\n",
    "p = C.shape[1] // 2   # Split column at position 512\n",
    "\n",
    "# Split matrices B^T and C\n",
    "B1 = B[:p, :]  # First 4 rows of transposed B\n",
    "B2 = B[p:, :]  # Remaining rows of transposed B\n",
    "C1 = C[:, :p]\n",
    "C2 = C[:, p:]\n",
    "\n",
    "# Perform sub-GEMMs\n",
    "Y1 = alpha * np.dot(A, B1.T) + beta * C1\n",
    "Y2 = alpha * np.dot(A, B2.T) + beta * C2\n",
    "\n",
    "# Concatenate the results\n",
    "Y = np.hstack((Y1, Y2))\n",
    "\n",
    "#print(\"Matrix Y after combining sub-GEMMs:\\n\", Y)\n",
    "Y0= alpha * np.dot(A, B.T) + beta * C\n",
    "assert_allclose(Y0,Y,rtol=1e-0)\n",
    "\n",
    "# Print the shapes of Y1 and Y2\n",
    "print(\"Shape of Y1:\", Y1.shape)\n",
    "print(\"Shape of Y2:\", Y2.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "print(\"Shape of Y0:\", Y0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import helper, TensorProto, numpy_helper\n",
    "import numpy as np\n",
    "\n",
    "# Define the inputs and outputs for the Gemm node\n",
    "input_A = helper.make_tensor_value_info('A', TensorProto.FLOAT, A.shape)\n",
    "output_Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, Y0.shape)\n",
    "\n",
    "\n",
    "# Create initializers for B and C\n",
    "initializer_B = numpy_helper.from_array(B, name='B')\n",
    "initializer_C = numpy_helper.from_array(C, name='C')\n",
    "\n",
    "# Create the Gemm node\n",
    "gemm_node = helper.make_node(\n",
    "    'Gemm',\n",
    "    inputs=['A', 'B', 'C'],\n",
    "    outputs=['Y'],\n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    transA=0,\n",
    "    transB=1\n",
    ")\n",
    "\n",
    "# Create the graph (GraphProto)\n",
    "graph_def = helper.make_graph(\n",
    "    [gemm_node],\n",
    "    'gemm_test',\n",
    "    [input_A],\n",
    "    [output_Y],\n",
    "    [initializer_B, initializer_C]\n",
    ")\n",
    "\n",
    "# Create the model (ModelProto)\n",
    "model_def = helper.make_model(graph_def, producer_name='gemm_example')\n",
    "\n",
    "# Save the model to a file\n",
    "onnx.save(model_def, 'onnx/gemm_model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import onnx\n",
    "import netron\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_model(model_file_name,itf='10.217.184.110',port=8098):\n",
    "    netron.start(file=model_file_name,address=(itf,port))\n",
    "    return port\n",
    "\n",
    "input_path=pathlib.Path(\"onnx/gemm_model.onnx\")\n",
    "#input_path=pathlib.Path(\"onnx/submodel_46_48.onnx\")\n",
    "onnx_model = onnx.load(input_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "def get_opset(model):\n",
    "    fields =model.opset_import\n",
    "    field=  fields[0]\n",
    "    return field.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{get_opset(onnx_model)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port=show_model(\"./\"+str(input_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import helper, TensorProto, numpy_helper\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "session = ort.InferenceSession('onnx/gemm_model.onnx')\n",
    "\n",
    "# Prepare the input dictionary\n",
    "input_dict = {'A': A}\n",
    "\n",
    "# Run the inference\n",
    "outputs = session.run(None, input_dict)\n",
    "\n",
    "assert_allclose(Y0,outputs[0],rtol=1e-0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx-mlir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
